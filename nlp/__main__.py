import fasttext as ft
import os
from pprint import pprint
import gensim
import wakati as wakati
import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆ
_model_gz_path = "data/cc.ja.300.vec.gz"
# ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆ
_model_path = "data/model.bin"
# å–ã‚Šå‡ºã™å“è©ãƒªã‚¹ãƒˆ
_select_conditions = ["å‹•è©", "åè©"]

# ãƒ¢ãƒ‡ãƒ«ã®DL
if not os.path.isfile(_model_gz_path):
    ft.download_model(_model_gz_path)

# ãƒ¢ãƒ‡ãƒ«ã®è§£å‡
if not os.path.isfile(_model_path):
    ft.save_model_from_gz(_model_path, _model_gz_path)

# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
wv = gensim.models.KeyedVectors.load(_model_path)

# firebaseå‘¨ã‚Š
cred = credentials.Certificate("data/hackukosen-firebase-adminsdk.json")
firebase_admin.initialize_app(cred)
db = firestore.client()

_select_conditions = ["å‹•è©", "åè©"]


def text_to_vectors(text: str) -> dict[str, list[tuple[str, float]]]:
    """æ–‡ç« ã‹ã‚‰ç‰¹å®šã®å“è©ã‚’å–ã‚Šå‡ºã—ã€ãã‚Œã®é¡ä¼¼å˜èªã‚’ãƒ™ã‚¯ãƒˆãƒ«ã§è¿”ã™"""
    # å˜èªã®åˆ†è§£
    words = list(set(wakati.text_to_word_by_conditions(text, _select_conditions)))
    ret: dict[str, list[tuple[str, float]]] = {}
    for word in words:
        # é¡ä¼¼ã—ãŸå˜èªã®å–å¾—
        try:
            ret[word] = ft.get_vector_from_words(ft.get_similar_words(wv, word))
        except:
            print(f"{word} ã¯fasttextã«ã‚ã‚Šã¾ã›ã‚“")
    return ret


def save_vector():
    # ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¤ºä»¶æ•°
    _vector_num = 5

    # æŠ•ç¨¿å–å¾—ä»¶æ•°
    _get_post_num = 3
    for uid in db.collections():
        topics = uid.document("topics").collections()
        for topic in topics:
            words_vectors = {}
            posts = topic.document("timeLine").collections()
            # postã¯collectionReference
            for post in list(posts)[-_get_post_num:]:
                text = list(post.stream())[-1].to_dict()["memo"]
                for words in text_to_vectors(text).items():
                    words_vectors[words[0]] = sorted(words[1], key=lambda x: x[1], reverse=True)[-_vector_num:]
            analytics = topic.document("analytics")
            ids = []
            for words in words_vectors.items():
                ids.append(words[0])
                for word in words[1]:
                    analytics.collection(words[0]).document(word[0]).set({"vector": word[1]})
            analytics.set({"id": ids})


# save_vector()
text_to_vectors("ã“ã‚“ã«ã¡ã¯ï¼ãŸãã•ã‚“ã®alphabetã‚„123ã‚’å«ã¿ã¾ã™ã€‚ã¾ãŸğŸ˜€ã‚‚å«ã¿ã¾ã™ã‚ˆï¼https://github.com/hacku-kosen-snct2022/nlp/issues/16")
